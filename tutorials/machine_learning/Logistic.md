上一篇介绍的线性回归是解决回归问题的，那么解决分类问题有什么模型呢？
从最简单分类问题的开始，也就是二分类问题，即y属于{0，1}，Logistic回归就是机器学习中比较基础的一个分类算法，对，名称里有回归，但是它就是分类算法，对于Logistic方法来说，我也把它记为以下三步：

	 1. 建立模型
	 2. 目标函数推导
	 3. 参数优化求解

----------
 - 模型建立
----------
先不加证明地给出一个模型，logistic/sigmoid函数：
$$
h_θ(x)=g(θ^Tx)=\frac {1}{1+e^{-θ^Tx}}
$$
这个函数有个性质
$$
\begin{aligned} 
g'(θ^Tx)&=(\frac {1}{1+e^{-θ^Tx}})'=\frac {e^{-x}}{(1+e^{-θ^Tx)^2}} \\
&=\frac {1}{1+e^{-θ^Tx}} (1-\frac{1}{1+e^{-θ^Tx}}) \\
&=g(θ^Tx)(1-g(θ^Tx))
\end{aligned}
$$


 - 目标函数推导
----------
假定模型服从二项分布，有：
$$
\begin{aligned} 
&p(y=1|x;θ)=h_θ(x) \\
&p(y=0|x;θ)=1-h_θ(x)
\end{aligned}
$$
则，综合写出概率密度函数为：
$$
p(y|x;θ)=h_θ(x)^{y}(1-h_θ(x))^{1-y}
$$
接下来，推导损失函数，这里损失函数有两种，以标签y的取值而区分。
 1、当y取值为{0，1}时：
$$
L(θ)=\prod_{i=1}^m p_{i}^{y_i}(1-p_i)^{1-y_i}
$$
取对数函数
$$
l(θ)=lnL(θ)=\sum_{i=1}^{m} {ln[ p_{i}^{y_i}(1-p_i)^{1-y_i}]}
$$

$$
令\ p_i=\frac {1}{1+e^{-f_i}}，其中 f_i= θ^Tx
$$

则
$$
l(θ)=\sum_{i=1}^m ln[ (\frac {1}{1+e^{-f_i}})^{y_i}(\frac {1}{1+e^{f_i}})^{1-y_i}]
$$


此时，损失函数为：

$$
loss(y_i,\hat {y_i})=-l(θ)=\sum_{i=1}^m[{y_i} ln ( {1+e^{-f_i}})+(1-y_i)(ln(1+e^{f_i}))]
$$
2、当y取值为{-1，1}时：
$$
L(θ)=\prod_{i=1}^m p_{i}^{(y_i+1)/2}(1-p_i)^{-(y_i-1)/2}
$$
同上处理，

$$
令\ p_i=\frac {1}{1+e^{-f_i}}\ ,其中 f_i= θ^Tx
$$
则
$$
\begin{aligned} 
   l(θ)&=lnL(θ)=\sum_{i=1}^m ln[ p_{i}^{y_i}(1-p_i)^{1-y_i}] \\
   &=\sum_{i=1}^m ln[ (\frac {1}{1+e^{-f_i}})^{(y_i+1)/2}(\frac {1}{1+e^{f_i}})^{-(y_i-1)/2}] \\
   loss(y_i,\hat {y_i})&=-l(θ)=\sum_{i=1}^m[{\frac {y_i+1}{2}} ln ( {1+e^{-f_i}})-\frac{y_i-1}{2}(ln(1+e^{f_i}))]\\
   &= \{ ^{{\sum_{i=1}^m}ln( {1+e^{-f_i}}),\ y_i=1}_{\sum_{i=1}^mln(1+e^{f_i}),\ y_i=-1}  \\
   &= \sum_{i=1}^m [ln(1+e^{-y_i f_i})]
 \end{aligned}
$$


此时，损失函数为：
$$
loss(y_i,\hat {y_i}) = \sum_{i=1}^m [ln(1+e^{-y_i f_i})]
$$


 - 参数优化求解
----------
以y取{0，1}为例，目标函数为
$$
loss(y_i,\hat {y_i})=-l(θ)=\sum_{i=1}^m[{y_i} ln ( {1+e^{-f_i}})+(1-y_i)(ln(1+e^{f_i}))]
$$
与线性回归一样的做法，求梯度，然后同梯度下降算法最小化loss函数，在此直接给出：
$$
\frac {\nabla J(θ)}{\nablaθj} =\sum_{i=1}^m(y^{(i)}-g(θ^Tx^{(i)}))x_{j}^{(i)}
$$
故得到Logistic回归参数的更新规则为：
$$
θj :=θj-\alpha \sum_{i=1}^m (h_{θ}(x^{(i)})-y^{(i)}))x_{j}^{(i)}
$$
同样，将m一次取k个样本更新做批量梯度下降：
$$
θj :=θj-\alpha \sum_{i=1}^k (h_{θ}(x^{(i)})-y^{(i)}))x_{j}^{(i)}
$$


思考
----------
Logistic回归与线性回归模型的参数更新规则在形式上一样，那么区别在哪呢？
区别就在连接函数g(z)：
1、若
$$
g(θ^Tx)=θ^Tx
$$
则为线性回归，它是假定模型服从正太分布推出来的。
2、若
$$
g(θ^Tx)=\frac {1}{1+e^{-θ^Tx}}
$$
则为logistic回归，它是假定模型服从二项分布推出来的。
其实这种结果是因为正太分布和二项分布都属于指数族分布，实际上logistic回归还是线性回归的范畴，它是广义的线性回归，是对数的线性模型。